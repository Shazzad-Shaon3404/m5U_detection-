{"cells":[{"cell_type":"markdown","metadata":{"id":"rFAOhcSOInOE"},"source":["# Library"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5185,"status":"ok","timestamp":1704185700293,"user":{"displayName":"Md. Shazzad Shaon 201-15-3404","userId":"02845614164894743268"},"user_tz":-360},"id":"GweHJnKfvUiC"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier  # sotring whole algo\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","import matplotlib.pyplot as plt\n","import matplotlib.image as pltimg\n","import seaborn as sns\n","from sklearn.ensemble import RandomForestClassifier\n","#tree #KNN #Log #GAussinab #Rf #SVC #SVM\n","from sys import argv\n","from sklearn import metrics\n","import numpy as np   # array\n","import pandas as pd  # read + plot\n","import sklearn.datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn import svm\n","from sklearn.metrics import classification_report,confusion_matrix #report\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.preprocessing import LabelEncoder #convert data\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn import svm\n","from sklearn.linear_model import RidgeClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.svm import LinearSVC\n","import xgboost as xgb\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import f1_score, precision_score, recall_score, log_loss, accuracy_score, matthews_corrcoef, roc_auc_score, cohen_kappa_score\n","import numpy as np\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","from sklearn.ensemble import StackingClassifier\n","import pandas as pd\n","from imblearn.over_sampling import ADASYN, SMOTE, SMOTENC\n","from imblearn.under_sampling import RandomUnderSampler, TomekLinks, ClusterCentroids, NearMiss, EditedNearestNeighbours\n","from imblearn.pipeline import Pipeline\n","from imblearn.datasets import make_imbalance\n","from sklearn.utils import class_weight\n","from sklearn.ensemble import RandomForestClassifier\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26655,"status":"ok","timestamp":1696331418856,"user":{"displayName":"Md. Shazzad Shaon 201-15-3404","userId":"02845614164894743268"},"user_tz":-360},"id":"1-i1iM8qj8lm","outputId":"4707a2de-4b30-4cfd-edb2-c6e94a0828ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_mthkKtx9Bn"},"outputs":[],"source":["tdf1= pd.read_csv('/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/ALL test F.csv')\n","\n","#train\n","df1= pd.read_csv(\"/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/ALL F features in one.csv\")\n","\n","X_train1 = df1.drop('Target', axis=1)\n","y_train1 = df1['Target']\n","#--------------------------------------------\n","\n","X_test1 = tdf1.drop('Target', axis=1)\n","y_test1 = tdf1['Target']\n","#--------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"A56WaEun8ikb"},"source":["# **GRU model cv**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"GIQ0_xm4gwpF","outputId":"785f2bad-27f8-4541-c6b7-447d38019ae3"},"outputs":[{"name":"stdout","output_type":"stream","text":["10/10 [==============================] - 1s 37ms/step\n","10/10 [==============================] - 1s 63ms/step\n","10/10 [==============================] - 1s 58ms/step\n","10/10 [==============================] - 0s 39ms/step\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import GRU, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization, Activation\n","from keras.metrics import AUC\n","from sklearn.metrics import accuracy_score, matthews_corrcoef, cohen_kappa_score, roc_auc_score, confusion_matrix, average_precision_score\n","\n","# Load the data\n","tdf1 = pd.read_csv('/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M test file.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M-traine.csv\")\n","\n","# Combine train and test data for cross-validation\n","X = pd.concat([df1.drop('Target', axis=1), tdf1.drop('Target', axis=1)], axis=0)\n","y = pd.concat([df1['Target'], tdf1['Target']], axis=0)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Create a GRU modelG with additional layers\n","modelG = Sequential()\n","\n","modelG.add(Conv1D(filters=64, kernel_size=3, input_shape=(X.shape[1], 1)))\n","modelG.add(BatchNormalization())\n","modelG.add(Activation('sigmoid'))\n","modelG.add(MaxPooling1D(pool_size=2))\n","\n","modelG.add(Conv1D(filters=128, kernel_size=3))\n","modelG.add(BatchNormalization())\n","modelG.add(Activation('relu'))\n","modelG.add(MaxPooling1D(pool_size=2))\n","\n","modelG.add(Conv1D(filters=64, kernel_size=3))\n","modelG.add(BatchNormalization())\n","modelG.add(Activation('relu'))\n","modelG.add(MaxPooling1D(pool_size=2))\n","\n","modelG.add(GRU(128, return_sequences=True))\n","modelG.add(GRU(64))\n","modelG.add(Dense(64))\n","modelG.add(BatchNormalization())\n","modelG.add(Activation('relu'))\n","modelG.add(Dropout(0.5))\n","\n","modelG.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the modelG\n","modelG.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', AUC()])\n","\n","# Reshape data for GRU\n","X = X.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Initialize K-fold cross-validation\n","n_splits = 10  # You can adjust the number of splits\n","kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","# Lists to store evaluation metrics\n","accuracies = []\n","mccs = []\n","kappas = []\n","specificities = []\n","sensitivities = []\n","aurocs = []\n","average_precisions = []\n","\n","# Perform K-fold cross-validation\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # Train the modelG\n","    modelG.fit(X_train, y_train, epochs=100, batch_size=128, verbose=0)\n","\n","    # Generate predictions on the test data\n","    y_pred = modelG.predict(X_test)\n","    y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, y_pred_binary)\n","    mcc = matthews_corrcoef(y_test, y_pred_binary)\n","    kappa = cohen_kappa_score(y_test, y_pred_binary)\n","    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = tp / (tp + fn)\n","    auroc = roc_auc_score(y_test, y_pred)\n","    average_precision = average_precision_score(y_test, y_pred)\n","\n","    # Append metrics to lists\n","    accuracies.append(accuracy)\n","    mccs.append(mcc)\n","    kappas.append(kappa)\n","    specificities.append(specificity)\n","    sensitivities.append(sensitivity)\n","    aurocs.append(auroc)\n","    average_precisions.append(average_precision)\n","\n","# Print average metrics\n","print(f'Average Accuracy: {np.mean(accuracies):.4f}')\n","print(f'Average MCC: {np.mean(mccs):.4f}')\n","print(f'Average Kappa Score: {np.mean(kappas):.4f}')\n","print(f'Average Specificity: {np.mean(specificities):.4f}')\n","print(f'Average Sensitivity: {np.mean(sensitivities):.4f}')\n","print(f'Average AUROC: {np.mean(aurocs):.4f}')\n","print(f'Average Average Precision: {np.mean(average_precisions):.4f}')\n"]},{"cell_type":"markdown","metadata":{"id":"kuUlgZRZqiWZ"},"source":["# CNN-bIlstm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwcMyO2X9MLj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Activation, LSTM, Bidirectional, Dense, Dropout\n","from keras.metrics import AUC\n","from sklearn.metrics import accuracy_score, matthews_corrcoef, cohen_kappa_score, roc_auc_score, confusion_matrix, average_precision_score\n","\n","# Load the data\n","# Load the data\n","tdf1 = pd.read_csv('/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M test file.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M-traine.csv\")\n","\n","# Combine train and test data for cross-validation\n","X = pd.concat([df1.drop('Target', axis=1), tdf1.drop('Target', axis=1)], axis=0)\n","y = pd.concat([df1['Target'], tdf1['Target']], axis=0)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Create a CNN-BiLSTM model\n","model = Sequential()\n","\n","model.add(Conv1D(filters=64, kernel_size=3, input_shape=(X.shape[1], 1)))\n","model.add(BatchNormalization())\n","model.add(Activation('sigmoid'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","model.add(Conv1D(filters=128, kernel_size=3))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","model.add(Conv1D(filters=64, kernel_size=3))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","\n","model.add(Bidirectional(LSTM(128, return_sequences=True)))\n","model.add(Bidirectional(LSTM(64)))\n","model.add(Dense(64))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', AUC()])\n","\n","# Reshape data for CNN-BiLSTM\n","X = X.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Initialize K-fold cross-validation\n","n_splits = 10  # You can adjust the number of splits\n","kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","# Lists to store evaluation metrics\n","accuracies = []\n","mccs = []\n","kappas = []\n","specificities = []\n","sensitivities = []\n","aurocs = []\n","average_precisions = []\n","\n","# Perform K-fold cross-validation\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # Train the model\n","    model.fit(X_train, y_train, epochs=100, batch_size=128, verbose=0)\n","\n","    # Generate predictions on the test data\n","    y_pred = model.predict(X_test)\n","    y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, y_pred_binary)\n","    mcc = matthews_corrcoef(y_test, y_pred_binary)\n","    kappa = cohen_kappa_score(y_test, y_pred_binary)\n","    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = tp / (tp + fn)\n","    auroc = roc_auc_score(y_test, y_pred)\n","    average_precision = average_precision_score(y_test, y_pred)\n","\n","    # Append metrics to lists\n","    accuracies.append(accuracy)\n","    mccs.append(mcc)\n","    kappas.append(kappa)\n","    specificities.append(specificity)\n","    sensitivities.append(sensitivity)\n","    aurocs.append(auroc)\n","    average_precisions.append(average_precision)\n","\n","# Print average metrics\n","print(f'Average Accuracy: {np.mean(accuracies):.4f}')\n","print(f'Average MCC: {np.mean(mccs):.4f}')\n","print(f'Average Kappa Score: {np.mean(kappas):.4f}')\n","print(f'Average Specificity: {np.mean(specificities):.4f}')\n","print(f'Average Sensitivity: {np.mean(sensitivities):.4f}')\n","print(f'Average AUROC: {np.mean(aurocs):.4f}')\n","print(f'Average Average Precision: {np.mean(average_precisions):.4f}')\n"]},{"cell_type":"markdown","metadata":{"id":"-eXH1EGmQQ6o"},"source":["# GRU+**LSTM**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlFP3uvfQf7w"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Activation, LSTM, GRU, Bidirectional, Dense, Dropout\n","from keras.metrics import AUC\n","from sklearn.metrics import accuracy_score, matthews_corrcoef, cohen_kappa_score, roc_auc_score, confusion_matrix, average_precision_score\n","\n","# Load the data\n","tdf1 = pd.read_csv('/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M test file.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M-traine.csv\")\n","\n","# Combine train and test data for cross-validation\n","X = pd.concat([df1.drop('Target', axis=1), tdf1.drop('Target', axis=1)], axis=0)\n","y = pd.concat([df1['Target'], tdf1['Target']], axis=0)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Create a model with both GRU and LSTM layers\n","modelGL = Sequential()\n","\n","modelGL.add(Conv1D(filters=64, kernel_size=3, input_shape=(X.shape[1], 1)))\n","modelGL.add(BatchNormalization())\n","modelGL.add(Activation('sigmoid'))\n","modelGL.add(MaxPooling1D(pool_size=2))\n","modelGL.add(Dropout(0.5))\n","\n","modelGL.add(Conv1D(filters=64, kernel_size=3))\n","modelGL.add(BatchNormalization())\n","modelGL.add(Activation('relu'))\n","modelGL.add(MaxPooling1D(pool_size=2))\n","\n","modelGL.add(GRU(256, return_sequences=True))\n","modelGL.add(LSTM(128, return_sequences=True))\n","modelGL.add(Dropout(0.5))\n","modelGL.add(GRU(64, return_sequences=True))\n","modelGL.add(LSTM(32, return_sequences=True))\n","modelGL.add(Flatten())\n","modelGL.add(Dense(64))\n","modelGL.add(BatchNormalization())\n","modelGL.add(Activation('relu'))\n","modelGL.add(Dropout(0.5))\n","\n","modelGL.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the modelGL\n","modelGL.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', AUC()])\n","\n","# Reshape data for GRU+LSTM\n","X = X.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Initialize K-fold cross-validation\n","n_splits = 10  # You can adjust the number of splits\n","kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","# Lists to store evaluation metrics\n","accuracies = []\n","mccs = []\n","kappas = []\n","specificities = []\n","sensitivities = []\n","aurocs = []\n","average_precisions = []\n","\n","# Perform K-fold cross-validation\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # Train the modelGL\n","    modelGL.fit(X_train, y_train, epochs=10, batch_size=128, verbose=0)\n","\n","    # Generate predictions on the test data\n","    y_pred = modelGL.predict(X_test)\n","    y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, y_pred_binary)\n","    mcc = matthews_corrcoef(y_test, y_pred_binary)\n","    kappa = cohen_kappa_score(y_test, y_pred_binary)\n","    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = tp / (tp + fn)\n","    auroc = roc_auc_score(y_test, y_pred)\n","    average_precision = average_precision_score(y_test, y_pred)\n","\n","    # Append metrics to lists\n","    accuracies.append(accuracy)\n","    mccs.append(mcc)\n","    kappas.append(kappa)\n","    specificities.append(specificity)\n","    sensitivities.append(sensitivity)\n","    aurocs.append(auroc)\n","    average_precisions.append(average_precision)\n","\n","# Print average metrics\n","print(f'Average Accuracy: {np.mean(accuracies):.4f}')\n","print(f'Average MCC: {np.mean(mccs):.4f}')\n","print(f'Average Kappa Score: {np.mean(kappas):.4f}')\n","print(f'Average Specificity: {np.mean(specificities):.4f}')\n","print(f'Average Sensitivity: {np.mean(sensitivities):.4f}')\n","print(f'Average AUROC: {np.mean(aurocs):.4f}')\n","print(f'Average Average Precision: {np.mean(average_precisions):.4f}')\n"]},{"cell_type":"markdown","metadata":{"id":"UfLSRNr1TxQb"},"source":["# RNN+CNN+GRU+LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIH2pIGRUAz2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, BatchNormalization, Activation, SimpleRNN, LSTM, GRU, Dense, Dropout, Input\n","from keras.metrics import AUC\n","from sklearn.metrics import accuracy_score, matthews_corrcoef, cohen_kappa_score, roc_auc_score, confusion_matrix, average_precision_score\n","\n","# Load the data\n","tdf1 = pd.read_csv('/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M test file.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M-traine.csv\")\n","\n","# Combine train and test data for cross-validation\n","X = pd.concat([df1.drop('Target', axis=1), tdf1.drop('Target', axis=1)], axis=0)\n","y = pd.concat([df1['Target'], tdf1['Target']], axis=0)\n","\n","# Standardize the data\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Create a model with SimpleRNN, LSTM, CNN, GRU, and additional layers\n","modelR = Sequential()\n","\n","# Convolutional layers (CNN)\n","modelR.add(Conv1D(filters=64, kernel_size=3, input_shape=(X.shape[1], 1)))\n","modelR.add(BatchNormalization())\n","modelR.add(Activation('relu'))\n","modelR.add(MaxPooling1D(pool_size=2))\n","modelR.add(Dropout(0.5))\n","\n","# RNN layers (SimpleRNN, LSTM, and GRU)\n","modelR.add(SimpleRNN(64, return_sequences=True))\n","modelR.add(BatchNormalization())\n","modelR.add(Dropout(0.5))\n","modelR.add(LSTM(64, return_sequences=True))\n","modelR.add(GRU(64))\n","modelR.add(Dropout(0.5))\n","modelR.add(Flatten())      # Flatten layer\n","\n","# Fully connected layers\n","modelR.add(Dense(64))\n","modelR.add(Activation('relu'))\n","modelR.add(Dropout(0.5))\n","\n","# Output layer\n","modelR.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the modelR\n","modelR.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', AUC()])\n","\n","# Reshape data for the modelR\n","X = X.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Initialize K-fold cross-validation\n","n_splits = 10  # You can adjust the number of splits\n","kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","# Lists to store evaluation metrics\n","accuracies = []\n","mccs = []\n","kappas = []\n","specificities = []\n","sensitivities = []\n","aurocs = []\n","average_precisions = []\n","\n","# Perform K-fold cross-validation\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # Train the modelR\n","    modelR.fit(X_train, y_train, epochs=10, batch_size=128, verbose=0)\n","\n","    # Generate predictions on the test data\n","    y_pred = modelR.predict(X_test)\n","    y_pred_binary = (y_pred > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    accuracy = accuracy_score(y_test, y_pred_binary)\n","    mcc = matthews_corrcoef(y_test, y_pred_binary)\n","    kappa = cohen_kappa_score(y_test, y_pred_binary)\n","    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = tp / (tp + fn)\n","    auroc = roc_auc_score(y_test, y_pred)\n","    average_precision = average_precision_score(y_test, y_pred)\n","\n","# Append metrics to lists\n","    accuracies.append(accuracy)\n","    mccs.append(mcc)\n","    kappas.append(kappa)\n","    specificities.append(specificity)\n","    sensitivities.append(sensitivity)\n","    aurocs.append(auroc)\n","    average_precisions.append(average_precision)\n","\n","# Print average metrics\n","print(f'Average Accuracy: {np.mean(accuracies):.4f}')\n","print(f'Average MCC: {np.mean(mccs):.4f}')\n","print(f'Average Kappa Score: {np.mean(kappas):.4f}')\n","print(f'Average Specificity: {np.mean(specificities):.4f}')\n","print(f'Average Sensitivity: {np.mean(sensitivities):.4f}')\n","print(f'Average AUROC: {np.mean(aurocs):.4f}')\n","print(f'Average Average Precision: {np.mean(average_precisions):.4f}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZXCRlr5aQgx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"d5Qyt5OvbA5n"},"source":["# ROC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKCr8nD1Oc4y"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GHjAetJEmu7"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import roc_curve, roc_auc_score, auc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Load your data\n","tdf1 = pd.read_csv('/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M test file.csv')\n","df1 = pd.read_csv(\"/content/drive/MyDrive/a new RNA m6A/m5U/FULL/FUll separete dataset/M-traine.csv\")\n","# Define more models and train them as needed\n","\n","# Assuming you have already trained your models\n","models = {\n","    'CNN-BiLSTM': model,    # Define your 'model' here\n","    'CNN-RNN-GRU-LSTM': modelR,  # Define your 'modelR' here\n","    'GRU-LSTM': modelGL,  # Define your 'modelGL' here\n","    'GRUpred-m5U': modelG    # Define your 'modelG' here\n","}\n","\n","# Split your data into features and labels\n","X = pd.concat([df1.drop('Target', axis=1), tdf1.drop('Target', axis=1)], axis=0)\n","y = pd.concat([df1['Target'], tdf1['Target']], axis=0)\n","\n","# Encode your labels if they are not binary (0, 1)\n","# This step is optional if your labels are already binary\n","label_encoder = LabelEncoder()\n","y = label_encoder.fit_transform(y)\n","\n","# Split your data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=1042)\n","\n","# Create a figure and axis for the ROC curve\n","plt.figure(figsize=(7, 4))\n","\n","# For each model in your dictionary\n","for model_name, model in models.items():\n","    # Make predictions on the test set\n","    if hasattr(model, 'predict_proba'):\n","        y_pred = model.predict_proba(X_test)[:, 1]  # Use predict_proba for probability estimates\n","    else:\n","        y_pred = model.predict(X_test)\n","        y_pred = (y_pred - y_pred.min()) / (y_pred.max() - y_pred.min())  # Manually calculate probabilities\n","\n","    # Calculate ROC curve and AUC\n","    fpr, tpr, _ = roc_curve(y_test, y_pred)\n","    roc_auc = auc(fpr, tpr)\n","\n","    # Plot the ROC curve\n","    plt.plot(fpr, tpr)\n","\n","# Plot the ROC curve for random guessing (AUC = 0.5)\n","plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n","\n","# Set axis labels and title\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.legend(loc='lower right')\n","# Save the resulting image as a JPEG file\n","plt.savefig('model1.jpg', format='jpg', dpi= 1500)\n","# Show the ROC curve plot\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPqm8ZCcSqH1xvEFDdU6iNy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}